/**
 * LLMAgnostic - API client for multiple AI model providers
 * This class provides a unified interface for interacting with different language model APIs
 */
import CONFIG from './CONFIG.js';

// Provider strategies
class ProviderStrategy {
    async testConnection(config) {
        throw new Error('testConnection method must be implemented');
    }
    
    async getCompletions(messages, apiConfig) {
        throw new Error('getCompletions method must be implemented');
    }
}

class MockProvider extends ProviderStrategy {
    async testConnection() {
        return { success: true };
    }
    
    async getCompletions(messages) {
        return new Promise((resolve) => {
            setTimeout(() => {
                const userMessage = messages[messages.length - 1];
                let response = "This is a mock response from the AI model.";
                
                if (userMessage.content.includes("Enter topic or keywords")) {
                    response = `# Generated Content\n\nThis is a generated response based on your keywords.\n\n## Main Points\n\n- First point about your topic\n- Second important consideration\n- Third interesting aspect\n\n## Further Information\n\nHere's more detailed information about what you requested. This is a mock response but in a real implementation, this would be generated by an AI model.`;
                } else {
                    response = `# Formatted Text\n\nThis is your text after formatting.\n\n## Highlights\n\n- Made it more concise\n- Improved readability\n- Added structure\n\nThe actual formatting would depend on the style you selected (social, blog, or minimal).`;
                }
                
                resolve({
                    role: "assistant",
                    content: response
                });
            }, 2000);
        });
    }
}

class HuggingFaceProvider extends ProviderStrategy {
    async testConnection(config) {
        if (!config.token) {
            throw new Error('Hugging Face API token is required');
        }
        
        try {
            const modelName = config.model || 'meta-llama/Llama-2-7b-chat-hf';
            const endpoint = `https://api-inference.huggingface.co/models/${modelName}`;
            
            const response = await fetch(endpoint, {
                method: 'GET',
                headers: {
                    'Authorization': `Bearer ${config.token.trim()}`
                }
            });
            
            if (!response.ok) {
                if (response.status === 401) {
                    throw new Error('Token di inferenza Hugging Face non valido. Assicurati di utilizzare un token di inferenza API, non un token di accesso utente standard.');
                } else {
                    throw new Error(`Errore API Hugging Face: ${response.status}. Verifica che il modello selezionato e le credenziali siano corretti.`);
                }
            }
            
            return { success: true, message: 'Connection to Hugging Face successful' };
        } catch (apiError) {
            return { success: false, error: apiError.message };
        }
    }
    
    async getCompletions(messages, apiConfig) {
        try {
            if (!apiConfig.token) {
                throw new Error('Hugging Face API token is required');
            }

            const token = apiConfig.token.trim();
            const userMessage = messages[messages.length - 1];
            const systemMessage = messages.find(msg => msg.role === 'system');
            
            const endpoint = apiConfig.endpoint || 
                    `https://api-inference.huggingface.co/models/${apiConfig.model || 'mistralai/Mistral-7B-Instruct-v0.2'}`;
            
            console.log(`Using model endpoint: ${endpoint}`);
            console.log(`Token length: ${token.length} characters`);
            
            let prompt = '';
            if (systemMessage) {
                prompt += `System: ${systemMessage.content}\n\n`;
            }
            
            for (let i = 0; i < messages.length - 1; i++) {
                const msg = messages[i];
                if (msg.role !== 'system') {
                    prompt += `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}\n\n`;
                }
            }
            
            prompt += `User: ${userMessage.content}\n\nAssistant: `;

            const response = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${token}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    inputs: prompt,
                    parameters: {
                        max_new_tokens: 1024,
                        temperature: 0.7,
                        top_p: 0.9,
                        do_sample: true
                    }
                })
            });
            
            if (!response.ok) {
                const errorText = await response.text();
                if (response.status === 401) {
                    throw new Error(`Autenticazione fallita. Assicurati di utilizzare un token di INFERENZA API Hugging Face, non un token di accesso utente standard. Error: ${response.status} ${errorText}`);
                } else if (response.status === 404) {
                    throw new Error(`Modello non trovato: "${apiConfig.model}". Verifica che il nome del modello sia corretto o scegli un altro modello.`);
                } else if (response.status === 503) {
                    throw new Error(`Il modello "${apiConfig.model}" non è attualmente disponibile. Potrebbe essere in fase di caricamento o non più supportato. Prova con un altro modello.`);
                } else {
                    throw new Error(`Errore API Hugging Face: ${response.status} ${errorText}`);
                }
            }

            const data = await response.json();
            
            let generatedText = '';
            if (Array.isArray(data) && data.length > 0) {
                generatedText = data[0]?.generated_text || '';
                const assistantPart = generatedText.split('Assistant: ').pop();
                generatedText = assistantPart || generatedText;
            } else {
                generatedText = data?.generated_text || '';
            }

            return {
                role: "assistant",
                content: generatedText
            };
        } catch (error) {
            console.error('Error calling Hugging Face API:', error);
            
            const isDevelopment = window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1';
            if (isDevelopment) {
                console.warn('Using fallback response due to API error');
                return {
                    role: "assistant",
                    content: "**API Error:** Unable to generate content. Please check your API configuration. The specific error was: " + error.message
                };
            }
            
            throw error;
        }
    }
    
    async checkModelAvailability(modelId, token) {
        if (!modelId || !token) {
            return { available: false, error: 'Model ID and token are required' };
        }
        
        const trimmedToken = token.trim();
        
        try {
            const inferenceEndpoint = `https://api-inference.huggingface.co/models/${modelId}`;
            
            const response = await fetch(inferenceEndpoint, {
                method: 'GET',
                headers: {
                    'Authorization': `Bearer ${trimmedToken}`
                }
            });
            
            if (response.ok) {
                return { available: true };
            }
            
            const statusErrors = {
                401: {
                    available: false, 
                    error: 'Token di inferenza API non valido. Assicurati di utilizzare un token di INFERENZA API Hugging Face, non un token di accesso utente standard.'
                },
                404: {
                    available: false, 
                    error: `Modello "${modelId}" non trovato. Verifica che l'ID del modello sia corretto.`
                },
                503: {
                    available: false,
                    error: `Il modello "${modelId}" non è attualmente disponibile. Potrebbe essere in fase di caricamento o non più supportato.`
                }
            };
            
            return statusErrors[response.status] || { 
                available: false, 
                error: `Errore nella verifica del modello: ${response.status}`
            };
        } catch (error) {
            return { 
                available: false, 
                error: `Errore durante la verifica del modello: ${error.message}`
            };
        }
    }
}

class OpenAIProvider extends ProviderStrategy {
    async testConnection(config) {
        if (!config.token) {
            throw new Error('OpenAI API token is required');
        }
        return { success: true, message: 'Connection to OpenAI successful' };
    }
    
    async getCompletions(messages, apiConfig) {
        try {
            if (!apiConfig.token) {
                throw new Error('OpenAI API token is required');
            }

            const endpoint = apiConfig.endpoint || 'https://api.openai.com/v1/chat/completions';

            const response = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${apiConfig.token}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    model: apiConfig.model || 'gpt-3.5-turbo',
                    messages: messages,
                    max_tokens: 1024,
                    temperature: 0.7
                })
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(`OpenAI API error: ${errorData.error?.message || response.statusText}`);
            }

            const data = await response.json();
            
            return {
                role: "assistant",
                content: data.choices?.[0]?.message?.content || 'No response generated'
            };
        } catch (error) {
            console.error('Error calling OpenAI API:', error);
            throw error;
        }
    }
}

class AzureProvider extends ProviderStrategy {
    async testConnection(config) {
        if (!config.token || !config.endpoint) {
            throw new Error('Azure API token and endpoint are required');
        }
        return { success: true, message: 'Connection to Azure successful' };
    }
    
    async getCompletions(messages, apiConfig) {
        try {
            if (!apiConfig.token || !apiConfig.endpoint) {
                throw new Error('Azure API token and endpoint are required');
            }

            const response = await fetch(`${apiConfig.endpoint}/openai/deployments/${apiConfig.model}/chat/completions?api-version=2023-05-15`, {
                method: 'POST',
                headers: {
                    'api-key': apiConfig.token,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    messages: messages,
                    max_tokens: 1024,
                    temperature: 0.7
                })
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(`Azure API error: ${errorData.error?.message || response.statusText}`);
            }

            const data = await response.json();
            
            return {
                role: "assistant",
                content: data.choices?.[0]?.message?.content || 'No response generated'
            };
        } catch (error) {
            console.error('Error calling Azure API:', error);
            throw error;
        }
    }
}

class AnthropicProvider extends ProviderStrategy {
    async testConnection(config) {
        if (!config.token) {
            throw new Error('Anthropic API token is required');
        }
        return { success: true, message: 'Connection to Anthropic successful' };
    }
    
    async getCompletions(messages, apiConfig) {
        try {
            if (!apiConfig.token) {
                throw new Error('Anthropic API token is required');
            }

            const endpoint = apiConfig.endpoint || 'https://api.anthropic.com/v1/messages';
            
            const formattedMessages = messages.filter(msg => msg.role !== 'system').map(msg => ({
                role: msg.role === 'assistant' ? 'assistant' : 'user',
                content: msg.content
            }));
            
            const systemMessage = messages.find(msg => msg.role === 'system');
            
            const response = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'x-api-key': apiConfig.token,
                    'anthropic-version': '2023-06-01',
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    model: apiConfig.model || 'claude-2',
                    messages: formattedMessages,
                    system: systemMessage?.content,
                    max_tokens: 1024
                })
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(`Anthropic API error: ${errorData.error?.message || response.statusText}`);
            }

            const data = await response.json();
            
            return {
                role: "assistant",
                content: data.content?.[0]?.text || 'No response generated'
            };
        } catch (error) {
            console.error('Error calling Anthropic API:', error);
            throw error;
        }
    }
}

class GitHubProvider extends ProviderStrategy {    async testConnection(config) {
        if (!config.token) {
            throw new Error('GitHub API token is required');
        }
        
        try {
            console.log('Testing GitHub connection with config:', { ...config, token: '***' });
            const endpoint = 'https://models.github.ai/catalog/models';
            
            console.log('Fetching from endpoint:', endpoint);
            
            // Use XMLHttpRequest as a workaround for CORS/OPTIONS issues
            return new Promise((resolve, reject) => {
                const xhr = new XMLHttpRequest();
                xhr.open('GET', endpoint, true);                xhr.setRequestHeader('Accept', 'application/vnd.github+json');
                xhr.setRequestHeader('Authorization', `Bearer ${config.token.trim()}`);
                xhr.setRequestHeader('X-GitHub-Api-Version', '2022-11-28');
                
                xhr.onload = function() {
                    console.log('GitHub API response status:', xhr.status);
                    
                    if (xhr.status >= 200 && xhr.status < 300) {
                        try {
                            const data = JSON.parse(xhr.responseText);
                            console.log('GitHub API response successful, received data:', data ? 'Data received' : 'No data');
                            resolve({ success: true, message: 'Connection to GitHub Models successful' });
                        } catch (error) {
                            console.error('Error parsing GitHub API response:', error);
                            reject(new Error(`Error parsing GitHub API response: ${error.message}`));
                        }
                    } else if (xhr.status === 401) {
                        reject(new Error('Token GitHub non valido. Verifica le tue credenziali.'));
                    } else {
                        console.error('GitHub API error response:', xhr.responseText);
                        reject(new Error(`Errore API GitHub: ${xhr.status}. Verifica che il token e le impostazioni siano corretti.`));
                    }
                };
                
                xhr.onerror = function() {
                    console.error('GitHub API request failed');
                    reject(new Error('Errore di connessione all\'API GitHub. Verifica la tua connessione di rete.'));
                };
                
                xhr.send();
            }).catch(apiError => {
                return { success: false, error: apiError.message };
            });
        } catch (apiError) {
            return { success: false, error: apiError.message };
        }
    }
      async getCompletions(messages, apiConfig) {
        try {
            if (!apiConfig.token) {
                throw new Error('GitHub API token is required');
            }

            const token = apiConfig.token.trim();
            const endpoint = 'https://models.github.ai/inference/chat/completions';
            const modelName = apiConfig.model || 'openai/gpt-4o-mini';
            
            console.log(`Using GitHub model: ${modelName}`);
            
            // Use XMLHttpRequest as a workaround for CORS/OPTIONS issues
            return new Promise((resolve, reject) => {
                const xhr = new XMLHttpRequest();
                xhr.open('POST', endpoint, true);                xhr.setRequestHeader('Accept', 'application/vnd.github+json');
                xhr.setRequestHeader('Authorization', `Bearer ${token}`);
                xhr.setRequestHeader('X-GitHub-Api-Version', '2022-11-28');
                xhr.setRequestHeader('Content-Type', 'application/json');
                
                const data = {
                    model: modelName,
                    messages: messages,
                    temperature: 0.7,
                    max_tokens: 1024
                };
                
                xhr.onload = function() {
                    if (xhr.status >= 200 && xhr.status < 300) {
                        try {
                            const responseData = JSON.parse(xhr.responseText);
                            resolve({
                                role: "assistant",
                                content: responseData.choices?.[0]?.message?.content || 'No response generated'
                            });
                        } catch (error) {
                            reject(new Error(`Error parsing GitHub API response: ${error.message}`));
                        }
                    } else if (xhr.status === 401) {
                        reject(new Error(`Autenticazione fallita. Verifica il tuo token GitHub. Error: ${xhr.status}`));
                    } else if (xhr.status === 404) {
                        reject(new Error(`Modello non trovato: "${modelName}". Verifica che il nome del modello sia corretto o scegli un altro modello.`));
                    } else {
                        reject(new Error(`Errore API GitHub: ${xhr.status} ${xhr.responseText}`));
                    }
                };
                
                xhr.onerror = function() {
                    reject(new Error('Errore di connessione all\'API GitHub. Verifica la tua connessione di rete.'));
                };
                
                xhr.send(JSON.stringify(data));
            });
        } catch (error) {
            console.error('Error calling GitHub Models API:', error);
            throw error;
        }
    }    async getAvailableModels(token) {
        if (!token) {
            return { success: false, error: 'GitHub API token is required' };
        }
        
        try {
            console.log('Getting available GitHub models with token');
            const endpoint = 'https://models.github.ai/catalog/models';
            
            console.log('Fetching from endpoint:', endpoint);
            
            // Use XMLHttpRequest as a workaround for CORS/OPTIONS issues
            return new Promise((resolve, reject) => {
                const xhr = new XMLHttpRequest();
                xhr.open('GET', endpoint, true);                xhr.setRequestHeader('Accept', 'application/vnd.github+json');
                xhr.setRequestHeader('Authorization', `Bearer ${token.trim()}`);
                xhr.setRequestHeader('X-GitHub-Api-Version', '2022-11-28');
                
                xhr.onload = function() {
                    console.log('GitHub models API response status:', xhr.status);
                    
                    if (xhr.status >= 200 && xhr.status < 300) {
                        try {
                            const modelsData = JSON.parse(xhr.responseText);
                            console.log('GitHub models API response successful, received models:', modelsData ? modelsData.length + ' models' : 'No models');
                            
                            // Formatta i modelli per l'uso nell'interfaccia utente
                            const formattedModels = modelsData.map(model => ({
                                value: model.id,
                                name: model.name || model.id,
                                publisher: model.publisher,
                                summary: model.summary,
                                tags: model.tags
                            }));
                            
                            resolve({ 
                                success: true, 
                                models: modelsData,
                                formattedModels: formattedModels
                            });
                        } catch (error) {
                            console.error('Error parsing GitHub models API response:', error);
                            reject(new Error(`Error parsing GitHub models API response: ${error.message}`));
                        }
                    } else if (xhr.status === 401) {
                        resolve({ 
                            success: false, 
                            error: 'Token GitHub non valido. Verifica le tue credenziali.'
                        });
                    } else {
                        console.error('GitHub models API error response:', xhr.responseText);
                        resolve({ 
                            success: false, 
                            error: `Errore API GitHub: ${xhr.status}. Verifica che il token e le impostazioni siano corretti.`
                        });
                    }
                };
                
                xhr.onerror = function() {
                    console.error('GitHub models API request failed');
                    resolve({ 
                        success: false, 
                        error: 'Errore di connessione all\'API GitHub. Verifica la tua connessione di rete.'
                    });
                };
                
                xhr.send();
            });
        } catch (error) {
            return { 
                success: false, 
                error: `Error retrieving models: ${error.message}`
            };
        }
    }
}

// Provider factory
class ProviderFactory {
    static getProvider(providerName) {
        const providers = {
            'mock': new MockProvider(),
            'huggingface': new HuggingFaceProvider(),
            'openai': new OpenAIProvider(),
            'azure': new AzureProvider(),
            'anthropic': new AnthropicProvider(),
            'github': new GitHubProvider()
        };
        
        return providers[providerName] || providers['mock'];
    }
}

class LLMAgnostic {
    static #apiConfig = {
        provider: 'mock', // default to mock
        token: '',
        model: '',
        endpoint: ''
    };

    /**
     * Configure the API settings
     * @param {Object} config - Configuration object
     * @param {string} config.provider - API provider (huggingface, openai, azure, mock, etc.)
     * @param {string} config.token - API token
     * @param {string} config.model - Model name for the provider
     * @param {string} config.endpoint - Custom endpoint URL if needed
     */
    static configure(config) {
        if (config) {
            this.#apiConfig = { ...this.#apiConfig, ...config };
        }
    }
    
    /**
     * Get current configuration
     * @returns {Object} Current API configuration
     */
    static getConfig() {
        return { ...this.#apiConfig };
    }
    
    /**
     * Test the connection to the selected API provider
     * @param {Object} config - Configuration to test
     * @returns {Promise<Object>} - Result of the test
     */
    static async testConnection(config) {
        const testConfig = config || this.#apiConfig;
        const provider = ProviderFactory.getProvider(testConfig.provider);
        
        try {
            return await provider.testConnection(testConfig);
        } catch (error) {
            return { success: false, error: error.message };
        }
    }

    static chat = {
        completions: {
            create: async ({ messages }) => {
                const apiConfig = LLMAgnostic.#apiConfig;
                const provider = ProviderFactory.getProvider(apiConfig.provider);
                  try {
                    return await provider.getCompletions(messages, apiConfig);
                } catch (error) {
                    console.error(`Error in ${apiConfig.provider} completions:`, error);
                    
                    // Instead of falling back to mock, propagate the error with additional info
                    const errorMsg = {
                        provider: apiConfig.provider,
                        model: apiConfig.model,
                        originalError: error.message,
                        content: "Si è verificato un errore durante la comunicazione con l'API."
                    };
                    
                    throw new Error(JSON.stringify(errorMsg));
                }
            }
        }
    };

    /**
     * Check if a specific Hugging Face model is available
     * @param {string} modelId - The model ID to check
     * @param {string} token - Hugging Face API token
     * @returns {Promise<Object>} - Result of the check
     */
    static async checkModelAvailability(modelId, token) {
        const huggingFaceProvider = new HuggingFaceProvider();
        return huggingFaceProvider.checkModelAvailability(modelId, token);
    }

    /**
     * Get available GitHub Models
     * @param {string} token - GitHub API token
     * @returns {Promise<Object>} - Result containing available models
     */
    static async getGitHubModels(token) {
        const githubProvider = new GitHubProvider();
        return githubProvider.getAvailableModels(token);
    }

    /**
     * Test connection to GitHub Models
     * @param {string} token - GitHub API token
     * @returns {Promise<Object>} - Result of the test
     */
    static async testGitHubConnection(token) {
        if (!token) {
            return { success: false, error: 'GitHub API token is required' };
        }
        
        try {
            // Test using the getGitHubModels method which already has error handling
            const result = await this.getGitHubModels(token);
            return {
                success: result.success,
                message: result.success ? 'Connection to GitHub Models successful' : result.error
            };
        } catch (error) {
            console.error('Error testing GitHub connection:', error);
            return { 
                success: false, 
                error: `Error testing GitHub connection: ${error.message}`
            };
        }
    }
}

export default LLMAgnostic;
